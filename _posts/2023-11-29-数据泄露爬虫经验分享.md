---
title: å®‰å…¨äººå‘˜çš„çˆ¬è™«æŠ€å·§
date: 2023-11-29 13:54:59 +0800
categories: 
tags:
  - çˆ¬è™«
permalink: /posts/id=75/
pin: false
---
## ä»selenium+undetected_chromedriverå¼€å§‹
æœ€è¿‘æœ‰çˆ¬å–æš—ç½‘å’Œå…¬ç½‘æ³„éœ²èµ„æºçš„çˆ¬è™«éœ€æ±‚ï¼Œç ”ç©¶å­¦ä¹ äº†ä¸€äº›seleniumçš„å®æˆ˜æŠ€å·§ï¼Œå½“ç„¶æˆ‘çˆ¬è™«ä¹Ÿä¸æ˜¯ä¸“ä¸šçš„ï¼Œæœ‰é”™è¯¯è¯·æŒ‡æ­£

æˆ‘å†™çˆ¬è™«é‡åˆ°çš„æœ€å¤§çš„é—®é¢˜åçˆ¬æ£€æµ‹ï¼Œå¤§éƒ¨åˆ†çš„ddoså¢™å¯ä»¥ç”¨undetected_chromedriverè¿™ä¸ªåº“æ¥å®ç°ç»•è¿‡
https://github.com/ultrafunkamsterdam/undetected-chromedriver
è¿™ä¸ªåº“ç¡®å®ä¹Ÿå¾ˆå¥½ç”¨ï¼Œå¾ˆå¤šåŸºç¡€çš„çˆ¬è™«æ£€æµ‹å’Œå¾ˆå¤šå°å‚çš„ddoså¢™éƒ½èƒ½ç»•è¿‡ï¼Œå°±æ˜¯éœ€è¦é…ä¸€ä¸ªå’Œæµè§ˆå™¨ç‰ˆæœ¬ç›¸åŒçš„chromedriveréº»çƒ¦äº†äº›

è¿™é‡Œè´´ä¸€ä¸ªä½¿ç”¨è¿™ä¸ªåº“çˆ¬å–hydraçš„æ³„éœ²ä¿¡æ¯çš„è„šæœ¬

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import undetected_chromedriver as uc
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup
import time
import pandas as pd
import re 
from datetime import datetime
from collections import Counter

options = webdriver.ChromeOptions()
options.add_argument('--ignore-certificate-errors') 
options.add_argument('--log-level=3')
driver = uc.Chrome(executable_path='D:/anaconda/chromedriver.exe',options = options)

wait = WebDriverWait(driver, 25)
base_url = 'https://hydramarket.org/Forums-vip-database?page={}&sortby=started'

df = pd.DataFrame(columns=['åºå·', 'æ•°æ®åç§°', 'æ•°æ®ç®€ä»‹', 'æ•°æ®å¤§å°', 'æ³„éœ²æ¸ é“', 'è·å–æ–¹å¼', 'æ•°æ®æ³„æ¼æ—¶é—´'])

# login
driver.get('https://hydramarket.org/member.php?action=login')

username = wait.until(EC.presence_of_all_elements_located((By.ID, 'username')))
passwd = wait.until(EC.presence_of_all_elements_located((By.ID, 'password')))
username[1].send_keys('e4l4')
passwd[1].send_keys('xxx')

login = wait.until(EC.presence_of_all_elements_located((By.XPATH, '/html/body/div[3]/form/div/div/div[5]/div[1]/input')))

login[0].click()
input("ç™»å½•åæŒ‰ Enter ç»§ç»­...")


# ç™»å½•å®Œæˆåï¼Œå¼€å§‹æŠ“å–
counter = 1  # ç”¨äºè®°å½•åºå·

# è®¾å®šæ—¥æœŸåŒºé—´
start_date = datetime(2023, 7, 1)
end_date = datetime(2023, 11, 21)

# æå–Sizeçš„æ­£åˆ™è¡¨è¾¾å¼
size_pattern = re.compile(r'\b\d+(?:[.,]\d+)?[kKmMgGbBkkKK]+\w*')

# # æå–Formatçš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œä»»ä½•åŒ…å«'format'çš„è¡Œéƒ½è¢«è§†ä¸ºæœ‰æ•ˆ
# format_pattern = re.compile(r'format\s*[:-]?\s*(.*)', re.IGNORECASE)
found = False

for page in range(1,89):  # è¿™é‡Œä»1å¼€å§‹ï¼Œå› ä¸ºå·²ç»ç™»å½•äº†
    if found:
        break
    url = base_url.format(page)
    try:
        driver.get(url)
        if 'googleseo_404_notfound' in driver.page_source:
                print(f"404é”™è¯¯:{url},{page}è·³è¿‡æ­¤é“¾æ¥")
                continue  # è·³è¿‡æ­¤æ¬¡

    except WebDriverException:
            print(f"æ— æ³•è®¿é—®é“¾æ¥ï¼š{url}ï¼Œè·³è¿‡æ­¤é“¾æ¥")
            continue  # è·³è¿‡æ­¤æ¬¡å¾ªç¯ï¼Œç»§ç»­çˆ¬å–ä¸‹ä¸€ä¸ªé“¾æ¥

    title_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.col a[style="font-size: 18px; font-weight: 500"]')))
    titles = [element.text for element in title_elements]

    link_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.col a[style="font-size: 18px; font-weight: 500"]')))
    links = [element.get_attribute("href") for element in link_elements]
    links = list(Counter(links).keys())
    print(links)

    for title, link in zip(titles, links):
        try:
            driver.get(link)
            if 'googleseo_404_notfound' in driver.page_source:
                print(f"404é”™è¯¯:{link}ï¼Œè·³è¿‡æ­¤é“¾æ¥")
                continue  
        except WebDriverException:
            print(f"æ— æ³•è®¿é—®é“¾æ¥ï¼š{link}ï¼Œè·³è¿‡æ­¤é“¾æ¥")
            continue
        print(link)
        time_element = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".col.align-self-center span.text-muted")))
        time_string = time_element[0].text
        if 'hour ago' in time_string:
            continue
        if 'hours ago' in time_string:
            continue

        if 'minutes ago' in time_string:
            continue
        if 'Yesterday' in time_string:
            continue
        
        print('ok1')
        postfix = " (This post was last modified:"
        if postfix in time_string:
            time_string = time_string.split(postfix)[0]

        time_info = datetime.strptime(time_string, '%m-%d-%Y, %I:%M %p')  # è§£ææ—¶é—´å­—ç¬¦ä¸²

        if time_info < start_date:
            found = True
            break
        if start_date <= time_info <= end_date:
            description_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.card.shadow-sm.mb-4.border-0.border.rounded.bg-white.p-5.m-0 span.mt-4')))
            description = description_element.text
            print('ok2')

            size_info_in_title = re.findall(size_pattern, title)  # å¯»æ‰¾æ ‡é¢˜ä¸­çš„Size
            size_info_in_description = re.findall(size_pattern, description)  # å¯»æ‰¾æè¿°ä¸­çš„Size
            size_info = list(set(size_info_in_title + size_info_in_description))  # åˆå¹¶ä¸¤è€…å¹¶åˆ é™¤é‡å¤å…ƒç´ 
            size = ', '.join(size_info) if size_info else ''

            # format_lines = [line for line in description.split('\n') if 'format' in line.lower()]
            # if format_lines:
            #     format_info = re.findall(format_pattern, format_lines[0])
            #     format = format_info[0].strip() if format_info else ''
            # else:
            #     format = ''

            df_row = pd.DataFrame([{'åºå·': counter, 'æ•°æ®åç§°': title, 'æ•°æ®ç®€ä»‹': description, 'æ•°æ®å¤§å°': size, 'æ³„éœ²æ¸ é“': 'hydra', 'è·å–æ–¹å¼': link, 'æ•°æ®æ³„æ¼æ—¶é—´': time_info.strftime('%Y-%m-%d %H:%M:%S')}])
            df = pd.concat([df, df_row], ignore_index=True)
            # print(f"åºå·: {counter}, æ•°æ®åç§°: {title}, æ•°æ®ç®€ä»‹: {description}, æ•°æ®å¤§å°: {size}, æ•°æ®æ ¼å¼: {format}, è·å–æ–¹å¼: {link}, æ•°æ®æ³„æ¼æ—¶é—´: {time_info.strftime('%Y-%m-%d %H:%M:%S')}")
            counter += 1
        # time.sleep(2)
    time.sleep(2)

try:
    driver.quit()
except Exception:
    pass


# df.replace(to_replace='[^ -~]+', value='', regex=True, inplace=True)
df.to_excel('test.xlsx',  index=False)

```

è¿™é‡Œæ˜¯æŒ‰åˆ›å»ºæ—¶é—´é¡ºåºæ’åºï¼Œå®ƒçš„urlæœ‰è§„å¾‹æ€§ï¼Œä¸»è¦çš„é€»è¾‘æ˜¯çˆ¬å–åˆ—è¡¨é¡µçš„æ ‡é¢˜å’ŒURLï¼Œç„¶åè¿›å»çˆ¬å–æ—¶é—´ï¼Œæ»¡è¶³æ—¶é—´éœ€æ±‚å†çˆ¬å–æè¿°å†…å®¹ï¼Œåç»­åœ¨æ ‡é¢˜å’Œæè¿°å†…å®¹ä¸­æå–sizeå­—æ®µï¼Œè¿˜æœ‰æ ¼å¼ï¼Œç„¶åå­˜å‚¨ä¸€æ¡ã€‚

å…¶ä¸­è·å–å…ƒç´ çš„ä¸»è¦çš„è¯­æ³•
```python
wait = WebDriverWait(driver, 25)
username = wait.until(EC.presence_of_all_elements_located((By.ID, 'username')))
```

é€»è¾‘æ˜¯è®¾ç½®ç­‰å¾…æ—¶é—´ï¼Œç­‰å¾…ç›´åˆ°å…ƒç´ å‡ºç°ï¼Œè¿™é‡Œçš„Byç”¨çš„æœ€å¤šçš„è¿˜æ˜¯ä»¥ä¸‹è¿™äº›æ–¹æ³•

```python
login = wait.until(EC.presence_of_all_elements_located((By.XPATH, '/html/body/div[3]/form/div/div/div[5]/div[1]/input')))

time_element = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".col.align-self-center span.text-muted")))
```

xpathç”¨æ¥æŒ‡å®šä¸€äº›å›ºå®šç‚¹æ¯”è¾ƒå¥½ç”¨ï¼Œæ¯”å¦‚ç™»å½•é¡µçš„ç™»å½•æŒ‰é’®ï¼Œcssé€‰æ‹©å™¨ç”¨æ¥æœç´¢ä¸€äº›é‡å¤æ ‡ç­¾æ¯”è¾ƒå¥½ç”¨
xpathå°±æ˜¯F12å³é”®ç›´æ¥å¤åˆ¶è·¯å¾„ï¼Œcssé€‰æ‹©å™¨çš„å†™æ³•åˆ™ä¸€èˆ¬ä»¥`.`å¼€å¤´
æ¯”å¦‚è¿™æ ·çš„æƒ…å†µ

```html
        <div class="post-block-text md1"> 
            STS Aviation Group is a service provider for the aviation industry. The company was founded in 1986 and is headquartered in Jensen Beach, Florida.        
        </div>
```

å°±åº”è¯¥æ˜¯

```python
wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".post-block-text.md1")))
```

ä¸åŒçš„classå±æ€§ä¹‹é—´ç”¨`.`é—´éš”ï¼Œä¸åŒçš„æ ‡ç­¾ä¹‹é—´ç”¨ç©ºæ ¼é”®é—´éš”ï¼Œç¬¬ä¸€ä¸ªæ ‡ç­¾ä¸ç”¨å†™æ ‡ç­¾åè€Œç”¨`.`ä»£æ›¿
å†è€…å°±æ˜¯ç™»å½•çš„é€»è¾‘ï¼Œç™»å½•ä¸»è¦æ˜¯ç”¨send_keysæ–¹æ³•è¾“å…¥æ•°æ®ï¼Œclickæ–¹æ³•å®Œæˆç‚¹å‡»

ä¸€èˆ¬æ¥è¯´æœ€åçš„æƒ…å†µä¹Ÿå¯ä»¥æ‰‹åŠ¨ç™»å½•ï¼Œä½†æ˜¯è¿™ä¸ªåº“é¢å¯¹cloudflareå°±æŸæ‰‹æ— ç­–äº†ï¼Œä¼šå‡ºç°ä¸€ç›´æ ¡éªŒçš„æƒ…å†µ

## æš—ç½‘çˆ¬è™«

æš—ç½‘çˆ¬è™«ä¸»è¦é’ˆå¯¹äºä¸€äº›éœ€è¦é€šè¿‡toræµè§ˆå™¨è®¿é—®çš„ç½‘ç«™è¿›è¡Œä½œä¸šã€‚ç”±äºtorå’ŒfirefoxåŸºäºåŒä¸€å†…æ ¸æ¶æ„ï¼ŒåŸºäºseleniumå¯¹äºfirefoxçš„æ”¯æŒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨seleninumå®ç°å¯¹æš—ç½‘çš„çˆ¬è™«ä½œä¸šã€‚
è¿™é‡Œä¸»è¦æ˜¯åœ¨windowsä¸‹çš„æ“ä½œï¼ŒlinuxåŒç†
é¦–å…ˆæ˜¯å…³äºå¦‚ä½•å¯åŠ¨
torçš„å¯åŠ¨éœ€è¦å€ŸåŠ©æ¢¯å­ï¼Œç„¶ååœ¨torç½‘ç»œè®¾ç½®é‡Œä½¿ç”¨socks5ä»£ç†ï¼Œæ·»åŠ ä¸ºæ¢¯å­çš„socks5ä»£ç†ç«¯å£å³å¯ï¼Œç„¶åä½ çš„torå°±ğŸ›«äº†
ç„¶åæ˜¯å¦‚ä½•é€šè¿‡è„šæœ¬å»è°ƒç”¨torï¼Œé…ç½®firfoxçš„é©±åŠ¨ï¼Œè¿™é‡Œéœ€è¦ç”¨åˆ°Geckodriverï¼Œæ˜¯ä¸€ä¸ªfirfoxçš„webdiver
åœ¨windowsä¸Šï¼Œtorè¿æ¥ç½‘ç»œåä¼šå¯åŠ¨åœ¨9150ç«¯å£ï¼Œæˆ‘ä»¬ç¼–è¾‘firefoxé…ç½®ï¼Œå¢åŠ socks5ä»£ç†ä¸ºtorçš„ç«¯å£
æˆ‘ä»¬å•ç‹¬å¯åŠ¨torè¿æ¥ä¸Šç½‘ç»œï¼Œæ­¤æ—¶å†è¿è¡Œè„šæœ¬èƒ½æ­£å¸¸æ‹‰èµ·firfoxï¼ŒåŒæ—¶èƒ½è®¿é—®æš—ç½‘äº†

![image.png](https://e4l4pic.oss-cn-beijing.aliyuncs.com/20231201165549.png)

å¯ä»¥ç”¨ä»¥ä¸‹ä»£ç åœ¨ç»ˆç«¯æµ‹è¯•è¿æ¥æƒ…å†µ

```shell
# torçˆ¬è™«éªŒè¯
curl --socks5 localhost:9150 --socks5-hostname localhost:9150 -s https://check.torproject.org/ | cat | grep -m 1 Congratulations | xargs
```

```python
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import time
import pandas as pd
import re
from datetime import datetime
from collections import Counter

gecko_driver_path = r'C:\Users\e4l4\Desktop\geckodriver-v0.33.0-win32\geckodriver.exe'

options = Options()
options.set_preference('permissions.default.image', 2)# æ— å›¾æ¨¡å¼
# options.add_argument('-headless')# æ— å¤´æ¨¡å¼
options.set_preference('network.proxy.type', 1)
options.set_preference('network.proxy.socks', '127.0.0.1')
options.set_preference('network.proxy.socks_port', 9150)
options.set_preference("network.proxy.socks_remote_dns", True)
  
service = Service(gecko_driver_path)
driver = webdriver.Firefox(service=service, options=options)
```

ç”±äºæš—ç½‘è®¿é—®ååˆ†ä¸ç¨³å®š(å¯èƒ½æœ‰ç½‘ç»œè´¨é‡çš„å› ç´ )ï¼Œæ‰€ä»¥æœ€å¥½é‡‡ç”¨æ— å¤´æˆ–æ— å›¾çš„æ¨¡å¼

è¿™é‡Œè´´ä¸€ä»½çˆ¬å–lockbit3çš„çˆ¬è™«è„šæœ¬

```python
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import time
import pandas as pd
import re 
from datetime import datetime
from collections import Counter

gecko_driver_path = r'C:\Users\e4l4\Desktop\geckodriver-v0.33.0-win32\geckodriver.exe'

options = Options()
options.set_preference('permissions.default.image', 2)
# options.add_argument('-headless') 
options.set_preference('network.proxy.type', 1)
options.set_preference('network.proxy.socks', '127.0.0.1')
options.set_preference('network.proxy.socks_port', 9150)
options.set_preference("network.proxy.socks_remote_dns", True)
service = Service(gecko_driver_path)
driver = webdriver.Firefox(service=service, options=options)

wait = WebDriverWait(driver, 70)

driver.get("http://lockbitapt6vx57t3eeqjofwgcglmutr3a35nygvokja5uuccip4ykyd.onion/")
df = pd.DataFrame(columns=["åºå·", "æ•°æ®åç§°", "æ•°æ®ç®€ä»‹", "æ•°æ®å¤§å°", "æ³„éœ²æ¸ é“", "è·å–æ–¹å¼", "æ•°æ®æ³„æ¼æ—¶é—´"])

with pd.ExcelWriter('lockbit.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
    df.to_excel(writer, index=False, sheet_name='Sheet1')
    record_blocks = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.post-block.good')))

    links = [element.get_attribute("href") for element in record_blocks]

    record_blocks = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.post-block.good div.post-title')))

    titles = [element.text for element in record_blocks]

    start_date = datetime(2023, 7, 1)
    end_date = datetime(2023, 11, 21)
    size_pattern = re.compile(r'\b\d+(?:[.,]\d+)?[kKmMgGbBkkKK]+\w*')
    counter = 1  # ç”¨äºè®°å½•åºå·


    for title, link in zip(titles[169:], links[169:]):
        if title == 'shakeys.com':
            break
        driver.get(link)
        print(link)
        # if 'hour ago' in time_string:
        #     continue
        # if 'hours ago' in time_string:
        #     continue

        # if 'minutes ago' in time_string:
        #     continue
        # if 'Yesterday' in time_string:
        #     continue
        time.sleep(1)
        time_element = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".uploaded-date-utc")))
        time_string = time_element[0].text
        time_info = datetime.strptime(time_string, '%d %b, %Y %H:%M UTC')
        print(time_info)
        
        if start_date <= time_info <= end_date:
            description_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.desc')))
            description = description_element.text
            print('ok2')
            
            try:
                wait = WebDriverWait(driver, 2)
                size_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.post-download-btn.file-download-btn span span')))
                size = size_element.text
                print(size)
            except TimeoutException:
                size_info_in_title = re.findall(size_pattern, title)
                size_info_in_description = re.findall(size_pattern, description)
                size_info = list(set(size_info_in_title + size_info_in_description))
                size = ', '.join(size_info) if size_info else ''

            df_row = pd.DataFrame([{'åºå·': counter, 'æ•°æ®åç§°': title, 'æ•°æ®ç®€ä»‹': description, 'æ•°æ®å¤§å°': size, 'æ³„éœ²æ¸ é“': 'Lockbit', 'è·å–æ–¹å¼': link, 'æ•°æ®æ³„æ¼æ—¶é—´': time_info.strftime('%Yå¹´%mæœˆ%dæ—¥')}])
            df = pd.concat([df, df_row], ignore_index=True)
            df.to_excel(writer, index=False, header=False, sheet_name='Sheet1')  # åœ¨æ¯æ¬¡è·å–åˆ°æ•°æ®åå³åˆ»è¿½åŠ åˆ° Excel æ–‡ä»¶
            # print(f"åºå·: {counter}, æ•°æ®åç§°: {title}, æ•°æ®ç®€ä»‹: {description}, æ•°æ®å¤§å°: {size}, æ•°æ®æ ¼å¼: {format}, è·å–æ–¹å¼: {link}, æ•°æ®æ³„æ¼æ—¶é—´: {time_info.strftime('%Y-%m-%d %H:%M:%S')}")  # åœ¨æ§åˆ¶å°è¾“å‡º
            counter += 1


driver.quit()

    # df.to_excel('lockbit.xlsx', index=False)

```


å’Œå‰é¢ç•¥æœ‰ä¸åŒçš„ç‚¹åœ¨äºï¼Œä¹‹å‰ä½¿ç”¨dfè¿›è¡Œå­˜å‚¨ï¼Œä½†æ˜¯å¾ˆå®¹æ˜“å‡ºç°æ–­è¿çš„æƒ…å†µå¯¼è‡´æ•°æ®å…¨éƒ¨ä¸¢å¤±ï¼Œæ‰€ä»¥å°±é‡‡ç”¨äº†çˆ¬ä¸€æ¡å­˜ä¸€æ¡çš„æ–¹å¼ã€‚å…¶ä½™é€»è¾‘éƒ½åŸºæœ¬ç›¸åŒï¼Œç”¨çš„åŸç”Ÿçš„seleniumã€‚

## DrissionPageå”¯ä¸€çœŸç¥

ç”±äºé‡åˆ°äº†cloudflareäººå·¥æ ¡éªŒçš„é—®é¢˜ï¼Œåœ¨çœ‹undetected_chromedriverçš„æ—¶å€™å‘ç°äº†ä¸€ä¸ªçˆ¬è™«é¡¹ç›®[DrissionPage](https://github.com/g1879/DrissionPage)ï¼Œæœ¬èº«è‡ªå¸¦cloudflareçš„ç»•è¿‡ï¼Œäºæ˜¯å¼€å§‹äº†è¿™ä¸ªæ¡†æ¶çš„å­¦ä¹ ã€‚
è¿™ä¸ªæ¡†æ¶åªèƒ½é©±åŠ¨Chromeå’ŒEdgeï¼Œæ‰€ä»¥æš‚æ—¶ä¸å¤ªèƒ½å¤„ç†æš—ç½‘çˆ¬è™«çš„é—®é¢˜
å…·ä½“çš„åŠŸèƒ½å¯ä»¥å‚è€ƒæ–‡æ¡£ï¼Œå†™çš„å¾ˆå…¨é¢äº†ï¼Œè¿™é‡Œè®°å½•ä¸€äº›æˆ‘çš„ç»éªŒ

```python
from DrissionPage import WebPage, ChromiumOptions, SessionOptions
from DrissionPage import ChromiumPage
import time
import pandas as pd
import re 
from bs4 import BeautifulSoup
from datetime import datetime


co = ChromiumOptions()
so = SessionOptions()
page = WebPage(driver_or_options=co, session_or_options=so)
df = pd.DataFrame(columns=['åºå·', 'æ•°æ®åç§°', 'æ•°æ®ç®€ä»‹', 'æ•°æ®å¤§å°', 'æ³„éœ²æ¸ é“', 'è·å–æ–¹å¼', 'æ•°æ®æ³„æ¼æ—¶é—´'])

# login
page.get('https://breachforums.is/member?action=login')

# time.sleep(20)
page.ele("@name=username").input("e4l4")
page.ele("@name=password").input("xxx")

# page.ele("xpath:/html/body/div/div[2]/div/div[2]/div[2]/div[1]/form/div[4]/input").click(by_js=True)
input("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•ï¼Œç„¶åæŒ‰ Enter ç»§ç»­...")
# ele.click

base_url = "https://breachforums.is/Forum-Databases?page={}&sortby=started"
# page.get("https://darkforums.me/Forum-Databases?page={}&sortby=started")

# page.get('https://breachforums.is/Thread-Victoria-University-Db-Psql-105Mb')
# description = page.ele("@class=post_body scaleimages").text
# print(description)
size_pattern = re.compile(r'\b\d+(?:[.,]\d+)?[kKmMgGbBkkKK]+\w*')

start_date = datetime(2023, 7, 1)
end_date = datetime(2023, 11, 21)

found = False
counter = 1 
for i in range(2,89):
    if found:
        break
    url = base_url.format(i)
    page.get(url)
    nodes = page.eles("@class= subject_new")
    times = page.eles("@class=forum-display__thread-date")

    titles = [node('tag:a').text for node in nodes]
    links = [node('tag:a').link for node in nodes]
    time_strings = [time.text for time in times]

    for title, link,time_string in zip(titles, links,time_strings):
        if 'hour ago' in time_string:
            continue
        if 'hours ago' in time_string:
            continue
        if 'minutes ago' in time_string:
            continue
        if 'Yesterday' in time_string:
            continue
        # 
        time_info = datetime.strptime(time_string, '%m-%d-%Y, %I:%M %p') 

        if time_info < start_date:
            found = True
            break
        if start_date <= time_info <= end_date:
            page.get(link)
            print(link)
            description = page.ele("@class=post_body scaleimages").text

            size_info_in_title = re.findall(size_pattern, title)  # å¯»æ‰¾æ ‡é¢˜ä¸­çš„Size
            size_info_in_description = re.findall(size_pattern, description)  # å¯»æ‰¾æè¿°ä¸­çš„Size
            size_info = list(set(size_info_in_title + size_info_in_description))  # åˆå¹¶ä¸¤è€…å¹¶åˆ é™¤é‡å¤å…ƒç´ 
            size = ', '.join(size_info) if size_info else ''

            df_row = pd.DataFrame([{'åºå·': counter, 'æ•°æ®åç§°': title, 'æ•°æ®ç®€ä»‹': description, 'æ•°æ®å¤§å°': size, 'æ³„éœ²æ¸ é“': 'breachforums', 'è·å–æ–¹å¼': link, 'æ•°æ®æ³„æ¼æ—¶é—´': time_info.strftime('%Y-%m-%d %H:%M:%S')}])
            df = pd.concat([df, df_row], ignore_index=True)
            counter += 1
        time.sleep(1)
    time.sleep(1)


# df.replace(to_replace='[^ -~]+', value='', regex=True, inplace=True)
df.to_excel('test.xlsx',  index=False)
```

è¿™ä¸ªæ¡†æ¶ä¸»è¦ç®€åŒ–äº†è·å–å…ƒç´ çš„ç¯èŠ‚ï¼Œå‡å°‘äº†ä»£ç é‡ï¼Œç”±äºæ¯”è¾ƒå°ä¼—ï¼Œå¯ä»¥ç»•è¿‡cloudflare
åŒæ—¶è¿˜æ”¯æŒçŠ¶æ€çš„åˆ‡æ¢å¯ä»¥åŒæ—¶ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼å’Œrequestsæ¨¡å¼ï¼Œä¸ºæœªæ¥ç»•è¿‡æœ‰äº†æ›´å¤šçš„å¯èƒ½æ€§
ç®€å•è®°å½•ä¸€ä¸‹ï¼Œå…·ä½“çš„è¯¦ç»†ä½¿ç”¨å¤§å®¶è¿˜æ˜¯çœ‹æ–‡æ¡£å§

```python
items = page.eles('t:h3')
for item in items[:-1]:
	lnk = item('tag:a')
	print(lnk.text, lnk.link)# æ‰“å°æ–‡æœ¬å’Œhrefå±æ€§
```

```python
# è·å– id ä¸º one çš„å…ƒç´ 
div1 = page.ele('#one')

# è·å– name å±æ€§ä¸º row1 çš„å…ƒç´ 
p1 = page.ele('@name=row1')

# è·å–åŒ…å«â€œç¬¬äºŒä¸ªdivâ€æ–‡æœ¬çš„å…ƒç´ 
div2 = page.ele('ç¬¬äºŒä¸ªdiv')

# è·å–æ‰€æœ‰divå…ƒç´ 
div_list = page.eles('tag:div')
```

`#`æ„æ€æ˜¯æŒ‰`id`å±æ€§æŸ¥æ‰¾å…ƒç´ 
`@`è¡¨ç¤ºæŒ‰å±æ€§åæŸ¥æ‰¾
`.`è¡¨ç¤ºclassæŸ¥æ‰¾
```python
# æŸ¥æ‰¾classå±æ€§ä¸ºp_clsçš„å…ƒç´ 
ele2 = ele1.ele('.p_cls')

# æŸ¥æ‰¾classå±æ€§'_cls'æ–‡æœ¬å¼€å¤´çš„å…ƒç´ 
ele2 = ele1.ele('.^_cls')  

ele2 = ele1.ele('@@name=row1@@class:cls')
# æŸ¥æ‰¾æ–‡æœ¬ä»¥â€œç¬¬äºŒâ€å¼€å¤´ä¸”classå±æ€§ä¸ºp_clsçš„å…ƒç´  []

ele2 = ele1.ele('@@text()^ç¬¬äºŒ@@class=p_cls')
```

## é™æ€çˆ¬è™«æ€è·¯
é™æ€çˆ¬è™«æ€è·¯ä¸»è¦æ˜¯ç”¨äºè§£å†³ç½‘ç»œæƒ…å†µè¾ƒå·®çš„æƒ…å†µï¼Œåˆä¸éœ€è¦æ›´æ·±å±‚æ¬¡çš„é“¾æ¥è®¿é—®ï¼Œåªéœ€è¦ä¸»é¡µçš„ä¿¡æ¯ï¼Œè¿™æ—¶å€™ä¸å¦¨
åœ¨æµè§ˆå™¨é‡Œå³é”®é¡µé¢ä¿å­˜ä¸‹æ¥(torä¹Ÿå¯ä»¥)

è¿™é‡Œè´´ä¸€ä¸ªä¹‹å‰å†™çš„lockbitçš„çˆ¬è™«ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæš—ç½‘çš„ä¾‹å­
```python
from bs4 import BeautifulSoup
import pandas as pd
import re
from datetime import datetime

with open("LockBit BLOG.htm", "r", encoding='utf-8') as f:
    contents = f.read()

soup = BeautifulSoup(contents, 'html.parser')

post_blocks = soup.find_all(class_=['post-block good', 'post-block bad'])

data = [] 
base_url = "http://lockbitapt6vx57t3eeqjofwgcglmutr3a35nygvokja5uuccip4ykyd.onion"

size_re = re.compile(r'\b\d+(\.\d+)?\s*[kKmMgGtTpP][bB]\b')

for i, block in enumerate(post_blocks):
    row = {}
    row['åºå·'] = i + 1
    row['æ•°æ®åç§°'] = block.find(class_='post-title').text.strip()
    row['æ•°æ®ç®€ä»‹'] = block.find(class_='post-block-text').text.strip() 
    size_match = size_re.search(row['æ•°æ®ç®€ä»‹'])
    row['æ•°æ®å¤§å°'] = size_match.group() if size_match else ''  
    row['æ¸ é“'] = ''  
    row['è·å–æ–¹å¼'] = base_url + block.get('onclick').split("'")[1]  
    
    leak_time = datetime.strptime(block.find(class_='updated-post-date').text.strip(), 'Updated: %d %b, %Y, %H:%M UTC')
    row['æ•°æ®æ³„éœ²æ—¶é—´'] = leak_time.strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    data.append(row)


illegal_char_re = re.compile(r'[\000-\037\177]')
for row in data:
    for key, value in row.items():
        if isinstance(value, str):
            row[key] = illegal_char_re.sub('', value)

df = pd.DataFrame(data)
df.to_excel("output.xlsx", index=False)

```

ä¸»è¦é€»è¾‘å°±æ˜¯ç”¨BeautifulSoupå»æŠ“å–å…ƒç´ ï¼Œç„¶åå¤„ç†æ”¾è¿›rowé‡Œå­˜å‚¨
## å­˜åœ¨çš„é—®é¢˜

- å¯¹äºä¸€äº›ç½‘é¡µå¡æ­»çš„æƒ…å†µï¼Œéœ€è¦å¢åŠ ä¸€äº›è‡ªåŠ¨é‡å¤è®¿é—®çš„ä»£ç 
- åˆ©ç”¨googleæµè§ˆå™¨è‡ªå¸¦çš„ç¿»è¯‘ï¼Œç¿»è¯‘åå†è¿›è¡Œçˆ¬è™«æ“ä½œï¼Œæ­£åˆ™åŒ¹é…ä¼šå¾ˆéº»çƒ¦ã€‚ä½†æ˜¯å¦‚æœçˆ¬å–å®Œå†åˆ©ç”¨ç¿»è¯‘APIè¿›è¡Œç¿»è¯‘ï¼Œé‡æ¯”è¾ƒå¤§çš„è¯æ›´éº»çƒ¦
- çˆ¬å–æ˜¯ä¸€æ¬¡ä¿å­˜ï¼Œä¸‡ä¸€å› ä¸ºç½‘ç»œåŸå› ä¸­æ–­ï¼Œå…¨ç›˜çš†æ— ã€‚ç›®å‰è¾ƒå¥½çš„è§£å†³åŠæ³•æ˜¯åˆ†æ®µä¿å­˜ï¼Œä¸çŸ¥é“æœ‰æ²¡æœ‰åˆ«çš„ä»£ç å†™æ³•ã€‚